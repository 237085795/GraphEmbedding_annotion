# from multiprocessing import cpu_count
# from joblib import Parallel, delayed
# import itertools
import numpy as np
#
#
# def create__file(file_path, msg):
#     f = open(file_path, "w")
#     f.write(msg)
#     f.close
#
#
# def func(a, in_put, b):
#     return a + in_put + b
#
#
# if __name__ == '__main__':
#     cpu__count = cpu_count()
# out = Parallel(n_jobs=5,verbose=50)(delayed(func)(1,i,1) for i in range(10))
# print(out)
# print("------------------------------------------")
# out1 = [i**3 for i in range(10)]
# print(out1)

# list1 = [[1, 2], [3, 4, 5], [6], [7, 8, 9, 10]]
# # print(np.array(list).shape)
# list2 = list(itertools.chain(*list1))
# print(list1)
# print(*list1)
# print(itertools.chain(*list1))
# print(list(itertools.chain(list1)))
# print(np.array(list2).shape)
# print(list(itertools.chain(*list1)))
import networkx as nx

if __name__ == '__main__':
    # G1 = nx.read_edgelist('../data/flight/usa-airports.edgelist', create_using=nx.DiGraph(), nodetype=int, data=[('weight', int)])
    G1 = nx.read_edgelist('../data/flight/europe-airports.edgelist', create_using=nx.DiGraph(), nodetype=int,
                          data=[('weight', int)])
    # G1 = nx.read_edgelist('../data/flight/brazil-airports.edgelist', create_using=nx.DiGraph(), nodetype=int,
    #                       data=[('weight', int)])
    print(G1.number_of_nodes())
    print(G1.number_of_edges())
#     def preprocess_nxgraph(graph):
#         node2idx = {}
#         idx2node = []
#         node_size = 0
#         for node in graph.nodes():
#             node2idx[node] = node_size
#             idx2node.append(node)
#             node_size += 1
#         return idx2node, node2idx
#     print(preprocess_nxgraph(G))

#
# def create_alias_table(area_ratio):
#     """
#
#     :param area_ratio: sum(area_ratio)=1
#     :return: accept,alias
#     """
#     l = len(area_ratio)
#     accept, alias = [0] * l, [0] * l
#     # print(accept,alias)
#     small, large = [], []
#     # print(area_ratio)
#     # print(np.array(area_ratio))
#     area_ratio_ = np.array(area_ratio) * l  # 均值归一化
#     # print(area_ratio_)
#     for i, prob in enumerate(area_ratio_):
#         if prob < 1.0:
#             small.append(i)
#         else:
#             large.append(i)
#
#     while small and large:
#         small_idx, large_idx = small.pop(), large.pop()
#         accept[small_idx] = area_ratio_[small_idx]
#         alias[small_idx] = large_idx
#         area_ratio_[large_idx] = area_ratio_[large_idx] - \
#             (1 - area_ratio_[small_idx])
#         if area_ratio_[large_idx] < 1.0:
#             small.append(large_idx)
#         else:
#             large.append(large_idx)
#
#     while large:
#         large_idx = large.pop()
#         accept[large_idx] = 1
#     while small:
#         small_idx = small.pop()
#         accept[small_idx] = 1
#
#     return accept, alias
#
# a={}
# a[1]=create_alias_table([0.1,0.3,0.4,0.2])
# print(a)
#

#
# print(G.edges.data())
# G.add_weighted_edges_from([(1, 3, 2),(1,2,3)],weight="heart")
# print(G.edges.data())
# # print(G.nodes, G.edges, G.number_of_nodes(), G.number_of_edges())
# # G.nodes[1]['size'] = 10
# # print(G.nodes.data())
# print(G.get_edge_data(1,2))
#
# plt.show()
# fig = plt.figure()
# ax = fig.add_subplot(232)
# ax = fig.add_subplot(233)
# ax.set(xlim=[0.5, 4.5], ylim=[-2, 8], title='An Example Axes', ylabel='Y-Axis', xlabel='X-Axis')
# nx.draw(G)
# plt.show()
# state = np.random.get_state()
# print(state[0])
# print(np.arange(10))


# from sklearn.preprocessing import MultiLabelBinarizer
#
# mlb = MultiLabelBinarizer()
# mlb.fit([1, 4, 9, 12, 15])


# print(mlb.classes_)
from sklearn.preprocessing import MultiLabelBinarizer

# mlb = MultiLabelBinarizer()
# Y=[['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['13'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['12'], ['12'], ['12'], ['12'], ['12'], ['12'], ['12'], ['12'], ['12'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['16'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['1'], ['9'], ['9'], ['9'], ['9'], ['9'], ['9'], ['9'], ['9'], ['9'], ['9'], ['9'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['5'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['11'], ['4'], ['4'], ['4'], ['4'], ['4'], ['4'], ['4'], ['4'], ['4'], ['4'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['3'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['0'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['7'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['2'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['14'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['6'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['10'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['15'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8'], ['8']]
# y = [('1', '3' ), ('3', '8')]
# mlb.fit(Y)
# print(mlb.classes_)
# print(y)
# print(np.random.permutation([1, 4, 9, 12, 15]))

# mlb.classes_
# top_k_list = [len(l) for l in Y]
# for i, k in enumerate(top_k_list):
#     print(i,k)
# print(op_k_list)

# dict={}
# dict.setdefault(Y[0][0],[8,9,10,11,12,13,14])
#
# for a,b in dict.items():
#     print(a,b)

# a=np.array([[1,1],[3,2],[3,3],[4,4],[5,5]])
# print(a[[1,2 ],0])

# import numpy  # 导入数据库
# from keras.datasets import mnist
# from keras.models import Sequential
# from keras.layers import Dense
# from keras.layers import Dropout
# from keras.utils import np_utils
#
# seed = 7  # 设置随机种子
# numpy.random.seed(seed)
#
# (X_train, y_train), (X_test, y_test) = mnist.load_data()  # 加载数据
#
# num_pixels = X_train.shape[1] * X_train.shape[2]
# X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
# X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
# # 数据集是3维的向量（instance length,width,height).对于多层感知机，模型的输入是二维的向量，因此这
# # 里需要将数据集reshape，即将28*28的向量转成784长度的数组。可以用numpy的reshape函数轻松实现这个过
# # 程。
#
# # 给定的像素的灰度值在0-255，为了使模型的训练效果更好，通常将数值归一化映射到0-1。
# X_train = X_train / 255
# X_test = X_test / 255
#
# # 最后，模型的输出是对每个类别的打分预测，对于分类结果从0-9的每个类别都有一个预测分值，表示将模型
# # 输入预测为该类的概率大小，概率越大可信度越高。由于原始的数据标签是0-9的整数值，通常将其表示成#0ne-hot向量。如第一个训练数据的标签为5，one-hot表示为[0,0,0,0,0,1,0,0,0,0]。
#
# y_train = np_utils.to_categorical(y_train)
# y_test = np_utils.to_categorical(y_test)
# num_classes = y_test.shape[1]
#
#
# # 现在需要做得就是搭建神经网络模型了，创建一个函数，建立含有一个隐层的神经网络。
# # define baseline model
# def baseline_model():
#     # create model
#     model = Sequential()
#     model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))
#     model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))
#     # Compile model
#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#     return model
#
#
# # 型的隐含层含有784个节点，接受的输入长度也是784（28*28），最后用softmax函数将预测结果转换为标签
# # 的概率值。
# # 将训练数据fit到模型，设置了迭代轮数，每轮200个训练样本，将测试集作为验证集，并查看训练的效果。
#
# # build the model
# model = baseline_model()
# # Fit the model
# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)
# # Final evaluation of the model
# scores = model.evaluate(X_test, y_test, verbose=0)


# class A:
# #     def __init__(self):
# #         self.x=1
# #         self.p()
# #         print("new")
# #     def p(self):
# #         print(self.x)
# #
# # a=A()

# import os
#
# def cmp_file(f1, f2):
#     st1 = os.stat(f1)
#     st2 = os.stat(f2)
#
#     # 比较文件大小
#     if st1.st_size != st2.st_size:
#         return False
#
#     bufsize = 8*1024
#     with open(f1, 'rb') as fp1, open(f2, 'rb') as fp2:
#         while True:
#             b1 = fp1.read(bufsize)  # 读取指定大小的数据进行比较
#             b2 = fp2.read(bufsize)
#             if b1 != b2:
#                 return False
#             if not b1:
#                 return True
#
# print(cmp_file('../data/wiki/Wiki_category.txt', '../data/wiki/wiki_labels.txt'))
# from metric.nmi import calc as NMI
# import numpy as np
# import math
# def NMI(A,B):
#     # len(A) should be equal to len(B)
#     total = len(A)
#     A_ids = set(A)
#     B_ids = set(B)
#     #Mutual information
#     MI = 0
#     eps = 1.4e-45
#     for idA in A_ids:
#         for idB in B_ids:
#             idAOccur = np.where(A==idA)
#             idBOccur = np.where(B==idB)
#             idABOccur = np.intersect1d(idAOccur,idBOccur)
#             px = 1.0*len(idAOccur[0])/total
#             py = 1.0*len(idBOccur[0])/total
#             pxy = 1.0*len(idABOccur)/total
#             MI = MI + pxy*math.log(pxy/(px*py)+eps,2)
#     # Normalized Mutual information
#     Hx = 0
#     for idA in A_ids:
#         idAOccurCount = 1.0*len(np.where(A==idA)[0])
#         Hx = Hx - (idAOccurCount/total)*math.log(idAOccurCount/total+eps,2)
#     Hy = 0
#     for idB in B_ids:
#         idBOccurCount = 1.0*len(np.where(B==idB)[0])
#         Hy = Hy - (idBOccurCount/total)*math.log(idBOccurCount/total+eps,2)
#     MIhat = 2.0*MI/(Hx+Hy)
#     return MIhat

# a=[{'2','1','3','4','5'},{'9','10','11'},{'6','7','8'}]
# b=[['1','2','3','4','5'],['6','7','8'],['9','10','11']]
#
#
# print(NMI(a,b))